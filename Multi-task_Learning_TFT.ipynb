{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf41e2c-a991-47c1-8c74-d04de66b4f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet, DeepAR, DecoderMLP, RecurrentNetwork, NBeats\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss, RMSE, MAPE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import os\n",
    "\n",
    "def set_seed(seed):\n",
    "    # seed init.\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # torch seed init.\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False # train speed is slower after enabling this opts.\n",
    "\n",
    "    # https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "    # avoiding nondeterministic algorithms (see https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b88f8-6372-43f9-a6ab-ab4eae069c24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Before making predictions, it is necessary to remove autocorrelation, such as converting it into volatility or return. In Github, we only provide the raw data.\n",
    "data = pd.read_csv('return_data_matrix')\n",
    "data.head(50)\n",
    "\n",
    "data = data.astype(dict(series=str))\n",
    "\n",
    "#Train, Validation, Test Split\n",
    "n = len(data)\n",
    "training_df = data[0:int(n * 0.8)]\n",
    "validation_df = data[int(n * 0.8):int(n * 0.9)]\n",
    "test_df = data[int(n * 0.9):]\n",
    "\n",
    "max_prediction_length = 1\n",
    "max_encoder_length = 5\n",
    "#training_cutoff = training_df[\"time_idx\"].max() - max_prediction_length*2\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    training_df,\n",
    "    #training_df[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=['high','low'],\n",
    "    group_ids=[\"series\"],\n",
    "    min_encoder_length=max_encoder_length,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    categorical_encoders={\"series\": NaNLabelEncoder().fit(training_df.series)},\n",
    "    static_categoricals=[\"series\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"high\",\n",
    "        \"low\",\n",
    "        \"close\",\n",
    "        \"open\",\n",
    "        \"volumek\",\n",
    "        \"gas\",\n",
    "        'oil',\n",
    "        'coal',\n",
    "        'power',\n",
    "        'stock',\n",
    "        'pos',\n",
    "        'neg',\n",
    "        'trends'\n",
    "    ],\n",
    "    add_relative_time_idx=True,\n",
    ")\n",
    "\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, validation_df, predict=False, stop_randomization=True)\n",
    "test = TimeSeriesDataSet.from_dataset(training, test_df, predict=False, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 64  # set this between 32 to 128\n",
    "\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "test_dataloader = test.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# and load the first batch\n",
    "x, y = next(iter(time_dataloader))\n",
    "print(\"x =\", x)\n",
    "print(\"\\ny =\", y)\n",
    "print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd75c2-fdef-4602-9090-d2ee1bf817ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    train_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=30,\n",
    "    max_epochs=300,\n",
    "    gradient_clip_val_range=(0.1, 0.1),\n",
    "    hidden_size_range=(8, 256),\n",
    "    hidden_continuous_size_range=(4, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=64, accelerator=\"cpu\"),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "    \n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb034b9-0110-433f-9635-4195e7760a5f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"train_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1000,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=64,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=study.best_trial.params['learning_rate'],\n",
    "    hidden_size=study.best_trial.params['hidden_size'],\n",
    "    attention_head_size=study.best_trial.params['attention_head_size'],\n",
    "    dropout=study.best_trial.params['dropout'],\n",
    "    hidden_continuous_size=study.best_trial.params['hidden_continuous_size'],\n",
    "    loss=QuantileLoss(),\n",
    "    #log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    optimizer=\"Ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "#{'gradient_clip_val': 0.1, 'hidden_size': 127, 'dropout': 0.1996489099681776, 'hidden_continuous_size': 61, 'attention_head_size': 3, 'learning_rate': 0.009964460308051648}\n",
    "#{'gradient_clip_val': 0.1, 'hidden_size': 79, 'dropout': 0.26240598317466385, 'hidden_continuous_size': 25, 'attention_head_size': 2, 'learning_rate': 0.0077954926055157}\n",
    "#{'gradient_clip_val': 0.1, 'hidden_size': 30, 'dropout': 0.12401596219667392, 'hidden_continuous_size': 18, 'attention_head_size': 4, 'learning_rate': 0.009616917298488703}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c3bf5-3e17-4415-9f70-c3a5dbd1f958",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"train_loss\", min_delta=1e-5, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n",
    "#pl.Trainer(deterministic=True)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=64,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    "\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.07,\n",
    "    hidden_size=115,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=44,\n",
    "    loss=QuantileLoss(),\n",
    "    #log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    optimizer=\"Ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    "\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee3f55-4728-4144-8c2f-fbfd7bf77b80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef9dcb-3a42-4a9e-bc5b-aca7f07605fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564090fe-4aec-4478-a5ba-37180d60560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_low = np.array(predictions.x['decoder_target'][1].cpu())\n",
    "actual_high = np.array(predictions.x['decoder_target'][0].cpu())\n",
    "pred_low = np.array(predictions.output[1].cpu())\n",
    "pred_high= np.array(predictions.output[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867407f7-6819-4271-9bb1-d1b0d3871ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the directional accuracy for interval predictions\n",
    "def interval_Dstat(actual_low, actual_high, pred_low, pred_high):\n",
    "    # Calculate the number of intervals\n",
    "    n = len(actual_low)\n",
    "    # Calculate the directional accuracy\n",
    "    correct_predictions = (np.array(actual_high * pred_high > 0) & np.array(actual_low * pred_low > 0)).astype(int)\n",
    "    interval_Dstat = np.sum(correct_predictions) / n  # n-1 because of np.diff reducing the length by 1\n",
    "    return interval_Dstat\n",
    "\n",
    "# Function to calculate interval evaluation metrics\n",
    "def IMAPE(actual_low, actual_high, pred_low, pred_high):\n",
    "    n = len(actual_low)\n",
    "    imape = (1 / (2 * n)) * np.sum(np.abs((actual_low - pred_low) / actual_low) + np.abs((actual_high - pred_high) / actual_high))\n",
    "    return imape\n",
    "\n",
    "def IRMSE(actual_low, actual_high, pred_low, pred_high):\n",
    "    n = len(actual_low)\n",
    "    irmse = 0.5 * (np.sqrt((1 / n) * np.sum((actual_low - pred_low) ** 2)) + np.sqrt((1 / n) * np.sum((actual_high - pred_high) ** 2)))\n",
    "    return irmse\n",
    "\n",
    "def IARV(actual_low, actual_high, pred_low, pred_high):\n",
    "    n = len(actual_low)\n",
    "    x_mean_low = np.mean(actual_low)\n",
    "    x_mean_high = np.mean(actual_high)\n",
    "    numerator = np.sum((actual_low[1:] - pred_low[:-1]) ** 2) + np.sum((actual_high[1:] - pred_high[:-1]) ** 2)\n",
    "    denominator = np.sum((actual_low[1:] - x_mean_low) ** 2) + np.sum((actual_high[1:] - x_mean_high) ** 2)\n",
    "    iarv = numerator / denominator\n",
    "    return iarv\n",
    "\n",
    "def UI(actual_low, actual_high, pred_low, pred_high):\n",
    "    n = len(actual_low)\n",
    "    x_mean_low = np.mean(actual_low)\n",
    "    x_mean_high = np.mean(actual_high)\n",
    "    numerator = np.sum((actual_low[1:] - pred_low[:-1]) ** 2) + np.sum((actual_high[1:] - pred_high[:-1]) ** 2)\n",
    "    denominator = np.sum((actual_low[1:] - x_mean_low) ** 2) + np.sum((actual_high[1:] - x_mean_high) ** 2)\n",
    "    ui = np.sqrt(numerator / denominator)\n",
    "    return ui\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate interval metrics\n",
    "def interval_metrics(actual_low, actual_high, pred_low, pred_high):\n",
    "    metrics = {\n",
    "        'IMAPE': IMAPE(actual_low, actual_high, pred_low, pred_high),\n",
    "        'IRMSE': IRMSE(actual_low, actual_high, pred_low, pred_high),\n",
    "        'IARV': IARV(actual_low, actual_high, pred_low, pred_high),\n",
    "        'UI': UI(actual_low, actual_high, pred_low, pred_high),\n",
    "        # 'Directional Accuracy High': Dstat(actual_high, pred_high),\n",
    "        # 'Directional Accuracy Low': Dstat(actual_low, pred_low),\n",
    "        'interval_Dstat':interval_Dstat(actual_low, actual_high, pred_low, pred_high)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Calculate and print the interval metrics\n",
    "metrics = interval_metrics(actual_low, actual_high, pred_low, pred_high)\n",
    "for metric, value in metrics.items():\n",
    "    print(f'{metric}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625c933-b1bf-488d-a649-13579e100324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "attentions = best_tft.predict(test_dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "interpretation = best_tft.interpret_output(attentions.output, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)\n",
    "\n",
    "print(interpretation['encoder_variables'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
